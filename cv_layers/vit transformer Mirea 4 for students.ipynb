{"cells":[{"cell_type":"markdown","source":["# Vit Transformer"],"metadata":{"id":"mfCxjGV5oTiJ"}},{"cell_type":"markdown","source":["## Модель"],"metadata":{"collapsed":false,"id":"D-FMYv-jwLiV"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3hL_6WoCwLiX"},"outputs":[],"source":["import torch\n","from torch import nn"]},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 10])\n","tensor([[-1.1127,  0.6661,  1.0794,  0.0196,  0.5441,  1.1445,  0.3761, -1.4601,\n","         -1.4382, -0.0627],\n","        [ 0.1915,  1.4360,  0.6060, -0.6318,  0.0714,  0.2601, -0.3320,  0.3403,\n","         -0.6557, -0.3798],\n","        [-0.4686, -0.2286, -1.2541,  0.5025,  0.5169, -1.3900,  0.1105,  0.8309,\n","         -0.5409,  0.1406],\n","        [-0.6098, -0.2844, -0.0605,  1.4607, -0.4396, -0.7302, -1.7419, -0.4694,\n","         -2.1952, -0.5802],\n","        [-1.8152,  2.1991,  1.0109,  0.4256,  0.4993,  0.9191, -0.4266, -1.4751,\n","          0.9440, -0.6904]])\n"]}],"source":["# Смоделируем данные\n","\n","n_features = 10  # Количество признаков\n","n_classes = 3  # Количество классов\n","batch_size = 5 \n","\n","data = torch.randn((batch_size, n_features))\n","print(data.shape)\n","print(data)"],"metadata":{"id":"GVgNwDyzwLiY","outputId":"d294d39a-69ae-49fc-c15f-8bd5bd230a27","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676295339666,"user_tz":-180,"elapsed":23,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# Зададим простую модель\n","model = nn.Linear(n_features, n_classes)"],"metadata":{"id":"GKVgvWaawLiZ"}},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 3])\n","tensor([[-0.3545,  0.2211, -0.1282],\n","        [-0.0493, -0.0335, -0.1272],\n","        [ 0.3399,  0.1628, -0.8176],\n","        [-0.8173,  0.4748, -0.1106],\n","        [-0.8670,  1.0389,  0.9653]], grad_fn=<AddmmBackward0>)\n"]}],"source":["# Применим модель к вектору\n","answer = model(data)\n","print(answer.shape)\n","print(answer)"],"metadata":{"id":"PyfujdNcwLia","outputId":"e9576182-8e25-44d9-c29d-e826d59108e2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676295339669,"user_tz":-180,"elapsed":23,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# Модель как наследник nn.Module\n","class SimpleNN(nn.Module):\n","    def __init__(self, n_features, n_classes):\n","        super().__init__()\n","\n","        self.lin = nn.Linear(n_features, n_classes)\n","\n","    def forward(self, x):\n","        return self.lin(x)"],"metadata":{"id":"oCsGwzuRwLib"}},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 3])\n","tensor([[-0.7035,  0.2531,  0.7247],\n","        [-0.3799, -0.1155,  0.6153],\n","        [ 0.5077,  0.1399, -0.8757],\n","        [ 0.7817,  0.9204, -0.3860],\n","        [-1.5674,  0.1022,  1.7614]], grad_fn=<AddmmBackward0>)\n"]}],"source":["# Попробуем применить модель в виде класса к данным\n","model = SimpleNN(n_features, n_classes)\n","\n","answer = model(data)\n","print(answer.shape)\n","print(answer)"],"metadata":{"id":"AJvKzV5ewLic","outputId":"71546ec3-2fe7-4e5b-f91a-ab7743ea4aac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676295339671,"user_tz":-180,"elapsed":20,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}}}},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                 [-1, 5, 3]              33\n","================================================================\n","Total params: 33\n","Trainable params: 33\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.00\n","----------------------------------------------------------------\n","None\n"]}],"source":["!pip install torchsummary\n","from torchsummary import summary\n","\n","model = SimpleNN(n_features, n_classes).cuda()\n","\n","# 5, 10\n","input_size = (batch_size, n_features)\n","print(summary(model, input_size))"],"metadata":{"id":"3I-SqCkrwLid","outputId":"02541efe-9458-4226-fca2-3d1a6579f655","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676295355427,"user_tz":-180,"elapsed":15771,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}}}},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 3])\n","tensor([[-1.5693,  0.9656,  0.4190],\n","        [-0.1636, -0.6284, -0.5807],\n","        [ 0.3587, -0.1695, -0.4409],\n","        [-1.4528,  0.1908,  0.1061],\n","        [-0.8333, -0.1868,  0.3930]], grad_fn=<AddmmBackward0>)\n"]}],"source":["# Модель как sequential\n","model = nn.Sequential(nn.Linear(n_features, n_classes))\n","\n","answer = model(data)\n","print(answer.shape)\n","print(answer)"],"metadata":{"id":"YIBvYYBewLie","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676295355428,"user_tz":-180,"elapsed":28,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}},"outputId":"424bca1b-3b43-4571-aec5-4e5a6d877b10"}},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 3])\n","tensor([[-0.8756, -0.2555,  0.2025],\n","        [-0.2768,  0.1934,  0.4638],\n","        [ 0.3753,  0.2621,  0.2184],\n","        [ 0.0898,  0.0724,  0.3421],\n","        [-1.1695,  0.7667,  0.3793]], grad_fn=<AddmmBackward0>)\n"]}],"source":["# Модель как nn.ModuleList\n","\n","model = nn.ModuleList([nn.Linear(n_features, n_classes)])\n","\n","# answer = model(data)\n","# print(answer.shape)\n","# print(answer)\n","\n","answer = model[0](data)\n","print(answer.shape)\n","print(answer)\n"],"metadata":{"id":"uBwzYlptwLif","outputId":"38685437-5dc9-4d09-e0b3-538e88bfefb7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676295355429,"user_tz":-180,"elapsed":24,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# Проверим параметры модели\n","class ParametersCheck(nn.Module):\n","    def __init__(self, n_features, n_classes):\n","        super().__init__()\n","\n","        self.lin = nn.Linear(n_features, n_classes)\n","        self.seq = nn.Sequential(nn.Linear(n_features, n_classes))\n","        self.module_list = nn.ModuleList([nn.Linear(n_features, n_classes)])\n","        self.list_of_layers = [nn.Linear(n_features, n_classes)]\n"],"metadata":{"id":"m5fE17SRwLig"}},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Параметр #1.\n","\ttorch.Size([3, 10])\n","Параметр #2.\n","\ttorch.Size([3])\n","Параметр #3.\n","\ttorch.Size([3, 10])\n","Параметр #4.\n","\ttorch.Size([3])\n","Параметр #5.\n","\ttorch.Size([3, 10])\n","Параметр #6.\n","\ttorch.Size([3])\n"]}],"source":["model = ParametersCheck(n_features, n_classes)\n","\n","for i, param in enumerate(model.parameters()):\n","    print(f'Параметр #{i + 1}.')\n","    print(f'\\t{param.shape}')"],"metadata":{"id":"pzvFgyhHwLih","outputId":"b00106da-382e-41b3-ca06-7a21921ed9c6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676295355431,"user_tz":-180,"elapsed":19,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}}}},{"cell_type":"markdown","source":["## ViT"],"metadata":{"collapsed":false,"id":"9_ccpqgpwLih"}},{"cell_type":"markdown","source":["![alt text](https://drive.google.com/uc?export=view&id=1J5TvycDPs8pzfvlXvtO5MCFBy64yp9Fa)"],"metadata":{"collapsed":false,"id":"O9Ck2xnvwLii"}},{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AFzQd5YDEbas","executionInfo":{"status":"ok","timestamp":1676295360765,"user_tz":-180,"elapsed":5349,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}},"outputId":"cb8f5ca1-0ca3-4549-9d2b-aa5998ee918e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting einops\n","  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.6.0\n"]}]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","from torch import nn\n","from torch import Tensor\n","from PIL import Image\n","from torchvision.transforms import Compose, Resize, ToTensor\n","from einops import rearrange, reduce, repeat\n","from einops.layers.torch import Rearrange, Reduce\n","from torchsummary import summary"],"metadata":{"id":"khe7vy_ZwLii"}},{"cell_type":"markdown","source":["![](https://amaarora.github.io/images/vit-01.png)"],"metadata":{"id":"cbPI9vsXDZH9"}},{"cell_type":"markdown","source":["## Часть 1. Patch Embedding, CLS Token, Position Encoding"],"metadata":{"id":"I7Au2Fd1FZbj"}},{"cell_type":"markdown","source":["![](https://amaarora.github.io/images/vit-02.png)"],"metadata":{"id":"YjbKwA7lGY3O"}},{"cell_type":"code","source":["# input image `B, C, H, W`\n","x = torch.randn(1, 3, 224, 224)\n","# 2D conv\n","conv = nn.Conv2d(3, 768, 16, 16)\n","conv(x).reshape(-1, 196).transpose(0,1).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tH4Nb22GeuS","executionInfo":{"status":"ok","timestamp":1676295360769,"user_tz":-180,"elapsed":41,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}},"outputId":"29e6667e-7ae9-4816-f279-f06dad01f17c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([196, 768])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["class PatchEmbedding(nn.Module):\n","    \"\"\" Image to Patch Embedding\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        self.img_size = (img_size, img_size)\n","        self.patch_size = (patch_size, patch_size)\n","        self.num_patches = (self.img_size[1] // self.patch_size[1]) * (self.img_size[0] // self.patch_size[0])\n","        self.patch_embeddings = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, image):\n","        # B, C, H, W = image.shape # assert\n","        patches = self.patch_embeddings(image).flatten(2).transpose(1, 2)\n","        #patches = patches.reshape(-1, 196).transpose(0,1)\n","        return patches"],"metadata":{"id":"WVwf4n1bwLik"}},{"cell_type":"code","source":["patch_embed = PatchEmbedding()\n","x = torch.randn(1, 3, 224, 224)\n","patch_embed(x).shape "],"metadata":{"id":"E57UzPBuE4qi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676297155023,"user_tz":-180,"elapsed":7,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}},"outputId":"3f4b4e98-627c-45f5-f3ae-f122b9d96061"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 196, 768])"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["![](https://amaarora.github.io/images/vit-03.png)"],"metadata":{"id":"JVUm-TJFGm6L"}},{"cell_type":"markdown","source":["## Часть 2. Transformer Encoder"],"metadata":{"id":"rUxuB53PFv1h"}},{"cell_type":"markdown","source":["![](https://amaarora.github.io/images/ViT.png)"],"metadata":{"id":"vkklM-fqFpa9"}},{"cell_type":"markdown","source":["![](https://amaarora.github.io/images/vit-07.png)"],"metadata":{"id":"G34WzminccX7"}},{"cell_type":"markdown","source":[],"metadata":{"id":"ACAqbCivDGsa"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n","        super().__init__()\n","\n","        self.seq = nn.Sequential(\n","          nn.Linear(in_features, hidden_features),\n","          nn.GELU(),\n","          nn.Linear(hidden_features, out_features),\n","          nn.GELU())\n","        \n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        \n","        x = self.drop(self.seq(x))\n","\n","        return x"],"metadata":{"id":"VPQts2WWdeYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.randn(1, 197,768)\n","mlp = MLP(768, 3072, 768)\n","out = mlp(x)\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LFxxcPoMf7IW","executionInfo":{"status":"ok","timestamp":1676299177646,"user_tz":-180,"elapsed":6,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}},"outputId":"22cb88de-9582-4216-9c77-fc36de91dc58"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 197, 768])"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., out_drop=0.):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        self.qkv = ...\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.out = ...\n","        self.out_drop = nn.Dropout(out_drop)\n","\n","    def forward(self, x):\n","        \n","        # Attention\n","        ...\n","\n","        ...\n","\n","        # Out projection\n","\n","        ...\n","\n","        return x\n"],"metadata":{"id":"4QnAW3rSc2OZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![](https://amaarora.github.io/images/vit-08.png)"],"metadata":{"id":"S_vgvLDbcjvi"}},{"cell_type":"code","source":["# attn = (q @ k.transpose(-2, -1)) * self.scale\n","# attn = attn.softmax(dim=-1)"],"metadata":{"id":"OukFkeXzdFpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.randn(1, 197, 768)\n","attention = Attention(768, 8)\n","out = attention(x)\n","out.shape"],"metadata":{"id":"8NeRHHJAgg5R"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["class Block(nn.Module):\n","    def __init__(self, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n","        super().__init__()\n","\n","        # Normalization\n","        ...\n","\n","        # Attention\n","        ...\n","\n","        # Dropout\n","        ...\n","\n","        # Normalization\n","        ...\n","\n","        # MLP\n","        ...\n","                \n","\n","    def forward(self, x):\n","        # Attetnion\n","        ...\n","\n","        # MLP\n","        ...\n","        return x"],"metadata":{"id":"K6e8y_YvwLik"}},{"cell_type":"code","source":["x = torch.randn(1, 197, 768)\n","block = Block(768, 8)\n","out = attention(x)\n","out.shape"],"metadata":{"id":"3aMihgfEhyql"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["В оригинальной реализации теперь используется [DropPath](https://github.com/rwightman/pytorch-image-models/blob/e98c93264cde1657b188f974dc928b9d73303b18/timm/layers/drop.py)"],"metadata":{"id":"BPBmiO5FhoN6"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, depth, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n","        super().__init__()\n","        self.blocks = nn.ModuleList([\n","            Block(dim, num_heads, mlp_ratio, drop_rate)\n","            for i in range(depth)])\n","\n","    def forward(self, x):\n","        for block in self.blocks:\n","            x = block(x)\n","        return x"],"metadata":{"id":"b1uO18VTwLil"}},{"cell_type":"code","source":["x = torch.randn(1, 197, 768)\n","block = Transformer(12, 768)\n","out = attention(x)\n","out.shape"],"metadata":{"id":"hIfp984oiBqc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![](https://amaarora.github.io/images/vit-06.png)"],"metadata":{"id":"GqUxpyv3cwNm"}},{"cell_type":"code","source":["from torch.nn.modules.normalization import LayerNorm\n","\n","class ViT(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000,\n","                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., \n","                 qkv_bias=False, drop_rate=0.,):\n","        super().__init__()\n","\n","        # Присвоение переменных\n","        \n","\n","        # Path Embeddings, CLS Token, Position Encoding\n","        self.patch_embed = PatchEmbedding(img_size=img_size, \n","                                               patch_size=patch_size,\n","                                               in_chans=in_chans,\n","                                               embed_dim=embed_dim)\n","        \n","        num_patches = self.patch_embed.num_patches\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n","\n","        # Transformer Encoder\n","        \n","        \n","        # Classifier\n","     \n","\n","    def forward(self, x):\n","        B = x.shape[0]\n","      \n","        # Path Embeddings, CLS Token, Position Encoding\n","        x = self.patch_embed(x)\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x = x + self.pos_embed\n","\n","        # Transformer Encoder\n","    \n","\n","        # Classifier\n","      \n","\n","        return x"],"metadata":{"id":"Y9gyxdqQeFs6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.randn(1, 3, 224, 224)\n","vit = ViT()\n","out = vit(x)\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2lGhne8kjeYs","executionInfo":{"status":"ok","timestamp":1676298495804,"user_tz":-180,"elapsed":8,"user":{"displayName":"Somebody Else","userId":"08803507653978694405"}},"outputId":"9fd3fe91-0eaf-4a95-a9b0-a637c5e45934"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 197, 768])"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["# Домашнее задание"],"metadata":{"id":"4QbFtayBkp-c"}},{"cell_type":"markdown","source":["\n","1. Выбрать датасет для классификации изображений с размерностью 64x64+ \n","2. Обучить ViT на таком датасете.\n","3. Попробовать поменять размерности и посмотреть, что поменяется при обучении.\n","\n","\n","Примечание:\n","- Датасеты можно взять [тут](https://pytorch.org/vision/stable/datasets.html#built-in-datasets) или найти в другом месте.\n","- Из за того, что ViT учится медленно, количество примеров в датасете можно ограничить до 1к-5к."],"metadata":{"id":"6nZbwbK9kskc"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[{"file_id":"1_B9nKSxz_ZY_7pzJy5RKc9sugHmDJR62","timestamp":1676294727837},{"file_id":"1_DsA3OAtgP1WHurZ1iYc16PsVMKGjxEw","timestamp":1676272619185}]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}